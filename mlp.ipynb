{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fc7ead",
   "metadata": {},
   "source": [
    "# Exercise 2: Multi-Layer Perceptrons (MLPs) with PyTorch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ITI-THM/EiDL/blob/master/docs/1-uebung/Perceptron.ipynb)\n",
    "\n",
    "Welcome to this exercise on Multi-Layer Perceptrons (MLPs)! In this notebook, we'll implement MLPs using PyTorch to solve both simple problems like XOR and more complex tasks like digit recognition with MNIST.\n",
    "\n",
    "**ðŸ“– Learning Goals:**\n",
    "* Understand why MLPs are necessary for problems like XOR\n",
    "* Implement MLPs using PyTorch's neural network modules\n",
    "* Train models on both the XOR problem and the MNIST dataset\n",
    "* Visualize decision boundaries and model performance\n",
    "* Gain hands-on experience with PyTorch's data handling capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c6a30",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries for our exercises. We'll use PyTorch for building and training our neural networks, along with visualization tools like Matplotlib and Seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm  # Import tqdm for progress bar\n",
    "import seaborn as sns\n",
    "\n",
    "# For MNIST dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set visual style\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c65f1",
   "metadata": {},
   "source": [
    "This time it ccan be usefull to execute the training on a GPU. So with the next snippet we check if a GPU is present and if so we set it as our used device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9abb2",
   "metadata": {},
   "source": [
    "## Part 1: Solving the XOR Problem with PyTorch\n",
    "\n",
    "### The XOR Problem\n",
    "\n",
    "The XOR (exclusive OR) problem is a classic example that demonstrates the limitations of single-layer neural networks and the power of multi-layer networks:\n",
    "\n",
    "* Input [0, 0] â†’ Output 0\n",
    "* Input [0, 1] â†’ Output 1\n",
    "* Input [1, 0] â†’ Output 1\n",
    "* Input [1, 1] â†’ Output 0\n",
    "\n",
    "A single perceptron can only create a linear decision boundary (a straight line in 2D), which cannot separate the XOR pattern. MLPs overcome this by introducing one or more **hidden layers** between the input and output layers, allowing the network to learn non-linear decision boundaries.\n",
    "\n",
    "Let's first create our XOR dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR Dataset\n",
    "X_xor = torch.tensor([\n",
    "    # TODO\n",
    "], dtype=torch.float32)\n",
    "\n",
    "y_xor = torch.tensor([\n",
    "    # TODO\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"XOR dataset created:\")\n",
    "print(f\"Inputs shape: {X_xor.shape}\")\n",
    "print(f\"Outputs shape: {y_xor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0e10c",
   "metadata": {},
   "source": [
    "### Building an MLP for XOR with PyTorch\n",
    "\n",
    "Now, let's define a simple MLP model using PyTorch's neural network modules. For the XOR problem, we'll use:\n",
    "\n",
    "* **Input Layer:** 2 neurons (for the 2 inputs of XOR)\n",
    "* **Hidden Layer:** 2 neurons (allows learning non-linearity)\n",
    "* **Output Layer:** 1 neuron (for the binary output of XOR)\n",
    "* **Activation Function:** Sigmoid (outputs values between 0 and 1)\n",
    "\n",
    "Resources: https://docs.pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our MLP model for XOR\n",
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.Sigmoid()):\n",
    "        super(XOR_MLP, self).__init__()\n",
    "        # TODO: Define the layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Define the forward pass\n",
    "        return x\n",
    "\n",
    "# Create the model and move it to the available device\n",
    "model = XOR_MLP().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2cef9",
   "metadata": {},
   "source": [
    "### Training the XOR Model\n",
    "\n",
    "Now we'll train our MLP on the XOR problem using:\n",
    "* Mean Squared Error (MSE) as the loss function\n",
    "* Stochastic Gradient Descent (SGD) as the optimizer\n",
    "* Early stopping to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9032c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 20000\n",
    "learning_rate = 0.5\n",
    "loss_history = []\n",
    "\n",
    "# Loss function and optimizer\n",
    "# TODO: Choose a loss function and optimizer\n",
    "criterion = ?\n",
    "optimizer = ?\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 1000  # Number of epochs to wait for improvement\n",
    "min_delta = 1e-5  # Minimum change to qualify as improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Move data to device\n",
    "X_xor = X_xor.to(device)\n",
    "y_xor = y_xor.to(device)\n",
    "\n",
    "# Training loop\n",
    "print(\"--- Starting Training ---\")\n",
    "print(f\"Learning Rate: {learning_rate}, Epochs: {epochs}, Early Stopping Patience: {patience}\")\n",
    "\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    #TODO: Forward pass\n",
    "    \n",
    "\n",
    "    #TODO: Calculate loss\n",
    "    loss = ?\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    #TODO: Backward pass and optimize\n",
    "\n",
    "\n",
    "    # Update progress bar with current loss\n",
    "    if (epoch + 1) % 100 == 0:  # Update less frequently to avoid slowdown\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if loss.item() < best_loss - min_delta:\n",
    "        best_loss = loss.item()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "print(f\"Final loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308a1b0",
   "metadata": {},
   "source": [
    "Plot the loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot the loss history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ff975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "print(\"\\n--- Testing Trained Network ---\")\n",
    "with torch.no_grad():  # No need to track gradients during testing\n",
    "    final_predictions = model(X_xor)\n",
    "\n",
    "    for i in range(len(X_xor)):\n",
    "        input_data = X_xor[i].cpu().numpy()\n",
    "        target = y_xor[i].item()\n",
    "        prediction = final_predictions[i].item()\n",
    "        print(f\"Input: {input_data}, Target: {target}, Prediction: {prediction:.4f} (Rounded: {round(prediction)})\")\n",
    "\n",
    "# Self-checks\n",
    "print(\"\\n--- Running Self-Checks ---\")\n",
    "with torch.no_grad():\n",
    "    # Test rounded predictions match targets\n",
    "    final_predictions_rounded = torch.round(final_predictions).flatten().int().cpu().numpy()\n",
    "    expected_xor_outputs = y_xor.flatten().int().cpu().numpy()\n",
    "\n",
    "    assert np.array_equal(final_predictions_rounded, expected_xor_outputs), \\\n",
    "        f\"Test Failed: Final XOR predictions {final_predictions_rounded} do not match targets {expected_xor_outputs}\"\n",
    "\n",
    "    # Check if loss has decreased significantly\n",
    "    final_loss = loss_history[-1]\n",
    "    assert final_loss < 0.05, \\\n",
    "        f\"Test Failed: Final loss {final_loss:.4f} is too high. Expected < 0.05\"\n",
    "\n",
    "    print(\"âœ… All Self-Checks Passed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec1d96",
   "metadata": {},
   "source": [
    "### Visualizing the XOR Decision Boundary\n",
    "\n",
    "Let's visualize the decision boundary learned by our MLP. This will show how the network has created a non-linear boundary to separate the XOR classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = -0.5, 1.5  # Extend beyond the data points\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    h = 0.01  # Step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Flatten the grid for prediction\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.tensor(grid_points, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Make predictions with the model\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_tensor)\n",
    "        Z = Z.cpu().numpy().reshape(xx.shape)\n",
    "\n",
    "    # Create the contour plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    contour = plt.contourf(xx, yy, Z, levels=20, cmap=plt.cm.RdBu, alpha=0.8)\n",
    "    plt.colorbar(contour, label='Predicted Value')\n",
    "\n",
    "    # Plot decision boundary at 0.5 threshold\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='green', linestyles='--')\n",
    "\n",
    "    # Plot the data points\n",
    "    X_np = X.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "    plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np.flatten(), cmap=plt.cm.RdBu,\n",
    "                edgecolors='k', s=200, marker='o')\n",
    "\n",
    "    # Annotate the points\n",
    "    for i, label in enumerate(['(0,0)â†’0', '(0,1)â†’1', '(1,0)â†’1', '(1,1)â†’0']):\n",
    "        plt.annotate(label, (X_np[i, 0], X_np[i, 1]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('MLP Decision Boundary for XOR Problem')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(model, X_xor, y_xor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218561b4",
   "metadata": {},
   "source": [
    "## Part 2: MNIST Digit Classification with PyTorch\n",
    "\n",
    "Now that we've solved the XOR problem, let's move on to a more complex task: classifying handwritten digits using the MNIST dataset.\n",
    "\n",
    "### The MNIST Dataset\n",
    "\n",
    "MNIST is a collection of 70,000 grayscale images of handwritten digits (0-9), commonly used for benchmarking machine learning models:\n",
    "* Each image is 28x28 pixels\n",
    "* 60,000 images for training, 10,000 for testing\n",
    "* 10 classes (digits 0-9)\n",
    "\n",
    "PyTorch makes it easy to load and prepare this dataset for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for MNIST\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch Tensor (scales to [0, 1])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize with MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=True,\n",
    "                                     download=True,\n",
    "                                     transform=train_transform)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.85 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                         train=False,\n",
    "                                         download=True,\n",
    "                                         transform=test_transform)\n",
    "\n",
    "print(f\"MNIST dataset loaded.\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736c30b",
   "metadata": {},
   "source": [
    "### Visualizing MNIST Data\n",
    "\n",
    "Let's visualize some examples from the MNIST dataset to get a better understanding of what we're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show MNIST images\n",
    "def show_mnist_images(dataset, num_images=10):\n",
    "    # Create a dataloader to get batches\n",
    "    dataloader = DataLoader(dataset, batch_size=num_images, shuffle=True)\n",
    "\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    # Create a grid of images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Get the image and remove normalization for display\n",
    "        img = images[i].squeeze().cpu().numpy()\n",
    "        label = labels[i].item()\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some training examples\n",
    "print(\"Examples from the MNIST training dataset:\")\n",
    "show_mnist_images(train_dataset)\n",
    "\n",
    "# Analyze the distribution of digits in the training set\n",
    "def analyze_mnist_distribution(dataset):\n",
    "    # Create a dataloader to get all labels\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    all_labels = []\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "    # Count occurrences of each digit\n",
    "    unique, counts = np.unique(all_labels, return_counts=True)\n",
    "\n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(unique, counts)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Digits in MNIST Dataset')\n",
    "    plt.xticks(unique)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "    # Add count labels on top of each bar\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(unique[i], count + 100, str(count), ha='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAnalyzing the distribution of digits in the training dataset:\")\n",
    "analyze_mnist_distribution(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca23dfd",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6701c",
   "metadata": {},
   "source": [
    "1. Describe the distribution of digit classes in the MNIST dataset. Are there classes that are more or less frequently represented?\n",
    "2. Take a look at the sample images shown: Are there any anomalies or similarities in the representation of the digits?\n",
    "3. What pre-processing steps could you apply to the images before training a neural network? Give reasons for your suggestions.\n",
    "4. Let's assume that one digit is significantly less frequently represented in the data set than others. What impact could this have on the training and performance of a model?\n",
    "5. What options are there for checking whether the loaded data is correct and complete?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11709f",
   "metadata": {},
   "source": [
    "### Creating DataLoaders for MNIST\n",
    "\n",
    "For efficient training, we'll use PyTorch's `DataLoader` class to:\n",
    "* Load data in batches\n",
    "* Shuffle the training data\n",
    "* Enable parallel data loading\n",
    "\n",
    "DataLoaders are essential for handling large datasets like MNIST that don't fit entirely in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca86649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 64  # Common batch size, often a power of 2\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {batch_size}\")\n",
    "\n",
    "# Let's examine one batch from the trainloader\n",
    "dataiter = iter(train_loader)\n",
    "images_batch, labels_batch = next(dataiter)\n",
    "\n",
    "print(f\"\\n--- Example Batch ---\")\n",
    "print(f\"Shape of images batch: {images_batch.shape}\")  # Shape: [batch_size, channels, height, width]\n",
    "print(f\"Shape of labels batch: {labels_batch.shape}\")  # Shape: [batch_size]\n",
    "print(f\"First few labels in the batch: {labels_batch[:10]}\")\n",
    "\n",
    "# Visualize a batch\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = images_batch[i].squeeze().cpu().numpy()\n",
    "    label = labels_batch[i].item()\n",
    "\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e800b",
   "metadata": {},
   "source": [
    "### Building an MLP for MNIST\n",
    "\n",
    "Now, let's define an MLP for the MNIST classification task:\n",
    "\n",
    "* **Input Layer:** 784 neurons (28Ã—28 pixels flattened)\n",
    "* **Hidden Layers:** Two hidden layers with ReLU activation\n",
    "* **Output Layer:** 10 neurons (one for each digit class)\n",
    "\n",
    "We'll use ReLU activation for the hidden layers as it generally performs better than Sigmoid for deep networks by helping to mitigate the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5685002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLP(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.ReLU()):\n",
    "        super(MNIST_MLP, self).__init__()\n",
    "        # TODO: Define the correct input size and hidden layers\n",
    "        input_size = ?  \n",
    "        hidden_size1 = ?    \n",
    "        hidden_size2 = ?     \n",
    "        output_size = ?      \n",
    "\n",
    "        #TODO: Define the layers of the MLP\n",
    "        self.flatten = nn.Flatten()  # Flatten the input -> makes the 2d image into a 1d vector\n",
    "        \n",
    "        # Note: We don't apply Softmax here because nn.CrossEntropyLoss\n",
    "        # expects raw logits and applies softmax internally\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO: Define the forward pass\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model_mnist = MNIST_MLP().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(\"PyTorch MLP Model for MNIST:\")\n",
    "print(model_mnist)\n",
    "\n",
    "# Test forward pass with a dummy batch\n",
    "dummy_batch = torch.randn(batch_size, 1, 28, 28).to(device)  # Example batch\n",
    "output = model_mnist(dummy_batch)\n",
    "print(f\"\\nOutput shape for a dummy batch: {output.shape}\")  # Should be [batch_size, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659deb2",
   "metadata": {},
   "source": [
    "### Training the MNIST Model\n",
    "\n",
    "Now we'll train our MLP on the MNIST dataset using:\n",
    "* Cross Entropy Loss (appropriate for multi-class classification)\n",
    "* SGD with momentum as the optimizer\n",
    "* Early stopping based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36474a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = ?\n",
    "optimizer = ?\n",
    "\n",
    "# Training parameters\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3\n",
    "min_delta = 1e-4\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "print(\"--- Starting Training ---\")\n",
    "print(f\"Learning Rate: 0.01, Momentum: 0.9, Epochs: {epochs}, Early Stopping Patience: {patience}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model_mnist.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        #TODO: Forward pass\n",
    "        \n",
    "\n",
    "        #TODO: Calculate loss\n",
    "        loss = ?\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += target.size(0)\n",
    "        correct_train += predicted.eq(target).sum().item()\n",
    "\n",
    "        #TODO: Backward pass and optimize\n",
    "        \n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", \n",
    "                                acc=f\"{100.*correct_train/total_train:.2f}%\")\n",
    "\n",
    "    # Calculate average training metrics for the epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100. * correct_train / total_train\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model_mnist.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\")\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model_mnist(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += target.size(0)\n",
    "            correct_val += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", \n",
    "                                   acc=f\"{100.*correct_val/total_val:.2f}%\")\n",
    "\n",
    "    # Calculate average validation metrics for the epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct_val / total_val\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_acc_history.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_loss - min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model_mnist.state_dict(), 'best_mnist_model.pth')\n",
    "        print(f\"Model improved and saved!\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement for {counter} epochs\")\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6af03",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize the training and validation metrics to understand how our model learned over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc_history, label='Training Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.title('Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f642b1",
   "metadata": {},
   "source": [
    "### Evaluating the MNIST Model\n",
    "\n",
    "Now let's evaluate our trained model on the test dataset to see how well it generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_mnist.load_state_dict(torch.load('best_mnist_model.pth'))\n",
    "model_mnist.eval()\n",
    "\n",
    "# Test the model on the test dataset\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
    "\n",
    "    for data, target in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model_mnist(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        # Store predictions and targets for confusion matrix\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", \n",
    "                               acc=f\"{100.*correct/total:.2f}%\")\n",
    "\n",
    "# Calculate average test metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100. * correct / total\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Average Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfcc80",
   "metadata": {},
   "source": [
    "### Visualizing Model Performance\n",
    "\n",
    "Let's create a confusion matrix to see which digits our model struggles with the most.\n",
    "\n",
    "Resource: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b005f",
   "metadata": {},
   "source": [
    "### Visualizing Misclassifications\n",
    "\n",
    "Let's look at some examples that our model misclassified to understand its weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show misclassified examples\n",
    "def show_misclassified_examples(model, test_loader, num_examples=10):\n",
    "    model.eval()\n",
    "    misclassified_images = []\n",
    "    misclassified_labels = []\n",
    "    misclassified_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            # Find misclassified examples\n",
    "            incorrect_mask = pred.ne(target)\n",
    "\n",
    "            if incorrect_mask.any():\n",
    "                misclassified_idx = torch.where(incorrect_mask)[0]\n",
    "\n",
    "                for idx in misclassified_idx:\n",
    "                    if len(misclassified_images) < num_examples:\n",
    "                        misclassified_images.append(data[idx].cpu())\n",
    "                        misclassified_labels.append(target[idx].item())\n",
    "                        misclassified_preds.append(pred[idx].item())\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            if len(misclassified_images) >= num_examples:\n",
    "                break\n",
    "\n",
    "    # Plot misclassified examples\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(len(misclassified_images)):\n",
    "        img = misclassified_images[i].squeeze().numpy()\n",
    "        true_label = misclassified_labels[i]\n",
    "        pred_label = misclassified_preds[i]\n",
    "\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"True: {true_label}, Pred: {pred_label}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show misclassified examples\n",
    "print(\"Examples of misclassified digits:\")\n",
    "show_misclassified_examples(model_mnist, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771ded1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we've:\n",
    "\n",
    "1. Implemented an MLP in PyTorch to solve the XOR problem\n",
    "2. Visualized the non-linear decision boundary learned by our network\n",
    "3. Built a more complex MLP for MNIST digit classification\n",
    "4. Used PyTorch's data handling capabilities with Dataset and DataLoader\n",
    "5. Trained, validated, and tested our model\n",
    "6. Visualized model performance and analyzed results\n",
    "\n",
    "These skills form the foundation for working with more complex neural network architectures and datasets in the future.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further improve the MNIST model, you could:\n",
    "* Experiment with different network architectures (more/fewer layers, different sizes)\n",
    "* Try different activation functions (Leaky ReLU, ELU, etc.)\n",
    "* Implement regularization techniques (dropout, weight decay)\n",
    "* Use more advanced optimizers (Adam, RMSprop)\n",
    "* Apply data augmentation to increase the effective training set size\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
