{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-extended",
   "metadata": {},
   "source": [
    "# Implementierung eines Perzeptrons und Erweiterung zu einem MLP – Erweiterte Übung\n",
    "\n",
    "{button}`Open in Collab <https://colab.research.google.com/github/ITI-THM/EiDL>`\n",
    "\n",
    "In diesem Notebook erfolgt zunächst die schrittweise Implementierung eines einfachen Perzeptrons und dessen Erweiterung zu einem Multilayer Perzeptron (MLP). Anschließend erweitern wir die Übung in einem zweiten Schritt um folgende Aspekte:\n",
    "\n",
    "- **Visualisierung der Entscheidungsgrenzen** für Perzeptron und MLP\n",
    "- **Anwendung des MLPs auf einen realen Datensatz** (Iris – binäre Klassifikation)\n",
    "\n",
    "Zudem gibt es erweiterte Reflexionsfragen, die das Verständnis vertiefen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ziel-basics",
   "metadata": {},
   "source": [
    "## Teil 1: Basisimplementierung (Perzeptron und MLP)\n",
    "\n",
    "Ziele:\n",
    "\n",
    "- Ein Perzeptron von Grund auf implementieren und trainieren (z. B. zur Lösung der AND-Funktion).\n",
    "- Das Modell zu einem MLP erweitern, das das nicht-linear separierbare XOR-Problem lösen kann.\n",
    "- Den Backpropagation-Algorithmus und die Rolle nichtlinearer Aktivierungsfunktionen kennenlernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Für reproduzierbare Ergebnisse\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perzeptron-intro",
   "metadata": {},
   "source": [
    "## 1. Implementierung des Perzeptrons\n",
    "\n",
    "Das klassische Perzeptron ist ein einfacher linearer Klassifikator, der Eingaben gewichtet summiert und mittels einer Stufenfunktion entscheidet. Im Training werden die Gewichte anhand des Fehlers angepasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perzeptron-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, learning_rate=0.1):\n",
    "        # Gewichte initialisieren (weights[0] = Bias)\n",
    "        self.weights = np.random.randn(n_inputs + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return self.activation(summation)\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=10):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = label - prediction\n",
    "                self.weights[1:] += self.learning_rate * error * np.array(inputs)\n",
    "                self.weights[0] += self.learning_rate * error\n",
    "\n",
    "# Beispiel: Training für die AND-Funktion\n",
    "training_inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "labels = np.array([0, 0, 0, 1])\n",
    "\n",
    "perc = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "perc.train(training_inputs, labels, epochs=10)\n",
    "\n",
    "print(\"Perzeptron-Gewichte:\", perc.weights)\n",
    "print(\"Vorhersagen:\")\n",
    "for inputs in training_inputs:\n",
    "    print(f\"Eingabe {inputs} => {perc.predict(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlp-intro",
   "metadata": {},
   "source": [
    "## 2. Erweiterung zu einem Multilayer Perzeptron (MLP)\n",
    "\n",
    "Ein einzelnes Perzeptron kann nur lineare Probleme lösen. Für nichtlineare Probleme (wie XOR) wird ein MLP mit mindestens einer versteckten Schicht benötigt. Hier implementieren wir ein MLP mit einer versteckten Schicht und trainieren es mittels Backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
